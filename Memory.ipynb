{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EntregaFinal.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"EpNCFT7_DLaW","colab_type":"text"},"source":["# **Práctica de Reconocimiento de Formas**\n","\n","- **Alumno 1**: Carlos Jiménez Martín - 160190\n","- **Alumno 2**: Enrique González Rodríguez - 160329"]},{"cell_type":"markdown","metadata":{"id":"htPBNhb6B2qj","colab_type":"text"},"source":["# **1. Introducción**\n","En el presente documento se expone la práctica final que corresponde a la primera mitad de la asignatura de Reconocimiento de Formas. \n","\n","Primero se exponen el clasificador de la distancia euclídea y el estadístico bayesiano que han sido implementados a lo largo de la duración de esta primera parte.\n","\n","Otro apartado corresponde a la regularización de los datos, en el que se expone el algoritmo usado para poder llevar a cabo la propia regularización de las matrices de covarianzas.\n","\n","El siguiente apartado corresponde a las técnicas de evaluación de rendimiento que se han estudiado en la asignatura y su utilización para evaluar los clasificadores implementados anteriormente.\n","\n","Finalmente se presenta un caso real de reconocimiento de formas, un OCR, el cual se implementa mediante uno de los clasificadores implementados anteriormente y probando su precisión en diferentes escenarios."]},{"cell_type":"markdown","metadata":{"id":"RDAj9UCCDmV5","colab_type":"text"},"source":["# **2. Clasificador de la distancia Euclídea**\n","\n","El clasificador de la distancia Euclídea es un clasificador que se basa en calcular la distancia mínima del punto que se quiere clasificar a los respresentaantes de cada clase, escogiendo la clase que minimize dicha distancia.\n","\n","Para la implementación de este clasificador se ha utlizado la libreria _numpy_, y se ha realizado de forma que funcione de forma correcta para un problema de clasificación que involucre un número cualquiera de clases, datos y elementos de vector de características. Se ha utlizado el método llamado **_broadcasting_** para que las matrices con con tamaños diferentes puedan operarse de la manera más óptima posible.\n","\n","Se ha utlizado:\n","- **_np.mean_** para calcular los centroides de cada clase.\n","- **_np.linalg.norm_** para calcular la distancia euclídea de los puntos a clasificar, ante los representantes de cada clase.\n","- **_np.argmin_** para obtener la mínima distancia entre el dato a clasificar y una clase.\n","\n","En la fase **_fit_** se han calculado los centroides de las clases.\n","\n","En la fase **_predict_** se calcula la distancia del punto a clasificar a los centroides de las diferentes clases, escogiendo la clase cuya distancia sea la menor. "]},{"cell_type":"code","metadata":{"id":"YX49sQL6Fr3V","colab_type":"code","colab":{}},"source":["import numpy as np\n","from abc import abstractmethod\n","\n","class Classifier:\n","\n","    @abstractmethod\n","    def fit(self,X,y):\n","        pass\n","\n","    @abstractmethod\n","    def predict(self,X):\n","        pass\n","\n","class ClassifEuclid(Classifier):\n","    def __init__(self,labels=[]):\n","        \"\"\"Constructor de la clase\n","        labels: lista de etiquetas de esta clase\"\"\"\n","        self.labels=labels\n","        self.centroides = None\n","        pass\n","\n","    def fit(self,X,y):\n","        \"\"\"Entrena el clasificador\n","        X: matriz numpy cada fila es un dato, cada columna una medida\n","        y: vector de etiquetas, tantos elementos como filas en X\n","        retorna objeto clasificador\"\"\"\n","        \n","        self.centroides = np.array([np.mean(X[y == i], axis=0)for i in labels])\n","        return self\n","\n","    def predict(self,X):\n","        \"\"\"Estima el grado de pertenencia de cada dato a todas las clases \n","        X: matriz numpy cada fila es un dato, cada columna una medida del vector de caracteristicas. \n","        Retorna una matriz, con tantas filas como datos y tantas columnas como clases tenga\n","        el problema, cada fila almacena los valores pertenencia de un dato a cada clase\"\"\" \n","        \n","        return np.linalg.norm(self.centroides[:, np.newaxis] - X, axis=2)\n","    \n","    def pred_label(self,X):\n","        \"\"\"Estima la etiqueta de cada dato. La etiqueta puede ser un entero o bien un string.\n","        X: matriz numpy cada fila es un dato, cada columna una medida\n","        retorna un vector con las etiquetas de cada dato\"\"\"\n","        \n","        return self.labels[np.argmin(X, axis=0)]\n","    \n","    def num_aciertos(self,X,y):\n","        \"\"\"Cuenta el numero de aciertos del clasificador para un conjunto de datos X.\n","        X: matriz de datos clasificados\n","        y: vector de etiquetas correctas\"\"\"\n","        \n","        assert self.centroides is not None, \"Error: Debes entrenar primero el clasificador\"\n","        return (X == y).sum(), (X == y).mean() * 100"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HLalQCEvF5nx","colab_type":"text"},"source":["### **Resultados**\n","\n","| Base de datos | Número de aciertos | Porcentaje de aciertos |\n","| --- | --- | --- |\n","| **Iris**   | 139 | 92.667 |\n","| **Wine**   | 129 | 72.472 |\n","| **Cancer** | 507 | 89.104 |\n","| **MNIST**  | 48479 | 80.798 |\n","| **Isolet** | 6843 | 87.765 |\n","\n","El mayor resultado alcanzado es el que corresponde al dataset _Iris_, lo cual puede deberse a que la distancia entre las distintas clases puede identificarse fácilmente debido a que están correctamente dispersas. \n","\n","El menor resultado, a su vez, es el del dataset _Wine_. Al contrario que en _Iris_, una menor dispersión de las clases puede hacer que estas distancias sean más dudosas al estar las clases más solapadas entre sí, lo que hace que el clasificador prediga peor los resultados."]},{"cell_type":"markdown","metadata":{"id":"NGyrM_UnHHb4","colab_type":"text"},"source":["# **3. Clasificador Estadístico Bayesiano**\n","\n","El clasificador estadístico Bayesiano es un clasificador que tiene en cuanta la dispersión y la forma de las clases, es decir, modela la distribución de las muestras de una clase, al contrario que el clasificador de la distancia euclídea. Este clasificador sigue el teorema estadístico de Bayes, donde se cumple que: \n","\n","- $P(\\alpha_i|x) = \\frac{P(x|\\alpha_i)\\cdot P(\\alpha_i)}{P(x)}$\n","\n","siendo _$\\alpha_i$_ una clase del modelo de datos, y _x_ el dato a clasificar.\n","\n","El criterio de clasificación se basa en minimizar la probabilidad de error del clasificador, de manera que se elije aquella clase cuya probabilidad a posteriori sea máxima.\n","\n","La fórmula general del clasificador estadístico bayesiano para una clase con distribución gausiana es:\n","\n","- $f_i(x) = \\mathcal{N}(x|\\mu_i,Σ_i)\\cdot P(\\alpha_i)$\n","\n","Tomando $d_i(x) = \\ln{f_i(x)}$:\n","\n","- $d_i(x) = -\\frac{1}{2}\\ln{|Σ_i|} - \\frac{1}{2}(x-\\mu_i)^\\intercalΣ_i^{-1}(x-\\mu_i)+\\ln{P(\\alpha_i)}$\n","\n","donde $Σ_i$ es la matriz de covarianzas de la clase _i_, que muestra mediante una matriz cuadrada la relación de unas características con otras a través de sus varianzas y covarianzas.\n","\n","Para su implementación en Python se ha utilizado:\n","-  **_np.log_** para calcular los logaritmos neperianos de las probabilidades a priori y de las matrices de covarianzas.\n","- **_np.mean_** para calcular las medias de cada clase.\n","- **_np.cov_** para calcular las matrices de covarianzas.\n","- **_np.linalg.inv_** para calcular la inversa de las matrices de covarianzas.\n","- **_np.matmul_** para multiplicar matrices.\n","- **_np.expand_dims_** para añadir dimensiones a las matrices.\n","- **_np.squeeze_** para eliminar dimensiones de las matrices.\n","- **_np.unique_** para obtener los elementos únicos de un array.\n","\n","Se ha utilizado _broadcasting_ y compresión de listas para poder implementar el algoritmo.\n","\n","En la fase **_fit_** primero se calculan los logaritmos neperieanos de la probabilidad a priori de cada clase, despué se calcula el valor de la media _$\\mu$_ para cada clase. Seguidamente se calculan las matrices de covarianzas _$Σ$_ de cada clase y sus logaritmos neperianos, y finalmente se calcula la inversa de estas.\n","\n","En la fase **_predict_**, únicamente se calcula el grado de pertenencia de cada valor a cada clase, a partir de los parámetros calculados en el entrenamiento, mediante la formula $d_i(x)$ que se muestra mas arriba."]},{"cell_type":"code","metadata":{"id":"1uTGaQswTVBX","colab_type":"code","colab":{}},"source":["import numpy as np\n","from abc import abstractmethod\n","\n","class Classifier:\n","\n","    @abstractmethod\n","    def fit(self,X,y):\n","        pass\n","\n","    @abstractmethod\n","    def predict(self,X):\n","        pass\n","\n","class ClassifBayesiano(Classifier):\n","    def __init__(self):\n","        \"\"\"Constructor de la clase\n","        labels: lista de etiquetas de esta clase\"\"\"\n","        self.labels = None\n","        self.ln_apriories = None\n","        self.means = None\n","        self.ln_determinants = None\n","        self.inv_covs = None\n","        \n","    def fit(self,X,y):\n","        \"\"\"Entrena el clasificador. Dado que es un clasificador Gausiano Bayesiano, \n","        se aprenderán los parámetros de las gausianas de cada clase.\n","        X: matriz numpy cada fila es un dato, cada columna una característica\n","        y: vector de etiquetas, tantos elementos como filas en X\n","        retorna objeto clasificador\"\"\"\n","        assert X.ndim == 2 and X.shape[0] == len(y)\n","        \n","        # Contar cuantos ejemplos hay de cada etiqueta\n","        self.labels, labels_count = np.unique(y, return_counts=True)\n","        \n","        # Usando el contador de ejemplos de cada etiqueta, calcular el logaritmo neperiano de las probabilidades a-priori\n","        self.ln_apriories = np.log(labels_count/y.size)\n","        \n","        # Calcular para los ejemplos de cada clase, la media de cada una de sus características (centroide)\n","        self.means = np.array([np.mean(X[y == i], axis=0) for i in np.unique(y)])\n","        \n","        # Sustraer a los ejemplos de cada clase su media y calcular su matriz de covarianzas (puedes emplear compresión de listas) \n","        self.covs = np.array([np.cov(X[y==np.unique(y)[i]]-self.means[i], rowvar=False) for i in range(len(self.labels))])\n","        \n","        # Para cada una de las clases, calcular el logaritmo neperiano de su matriz de covarianzas (puedes emplear compresión de listas o la función map)\n","        self.ln_determinants = np.log(np.linalg.det(self.covs))\n","        \n","        # Para cada una de las clases, calcular la inversa de su matriz de covarianzas (puedes emplear compresión de listas o la función map)\n","        self.inv_covs = np.linalg.inv(self.covs)\n","\n","        return self\n","\n","    def predict(self,X):\n","        \"\"\"Estima el grado de pertenencia de cada dato a todas las clases \n","        X: matriz numpy cada fila es un dato, cada columna una medida del vector de caracteristicas. \n","        Retorna una matriz, con tantas filas como datos y tantas columnas como clases tenga\n","        el problema, cada fila almacena los valores pertenencia de un dato a cada clase\"\"\" \n","        assert self.means is not None, \"Error: The classifier needs to be fitted. Please call fit(X, y) method.\"\n","        assert X.ndim == 2 and X.shape[1] == self.means.shape[1]\n","\n","        # Resta la media de cada clase a cada ejemplo en X\n","        X_mean0 = np.array([X-self.means[i] for i in range(len(self.means))])\n","\n","        a = -(1/2)*self.ln_determinants\n","        b = np.matmul(np.expand_dims(X_mean0, 2), np.expand_dims(self.inv_covs,1))\n","        c = - (1/2)*np.matmul(b,  np.expand_dims(X_mean0, 3))\n","\n","        # Calcula el logaritmo de la función de decisión gausiana \n","        # Transparencias de Métodos paramétricos de clasificación: página 14:\n","        # -(1/2)ln|Sigma_i| - (1/2)*(x- mu_i)^T Sigma_i^-1 (x- mu_i) + lnP(alpha_i)\n","        grado_de_pertenencia = -(1/2)*self.ln_determinants - (1/2)*np.squeeze(np.matmul(np.matmul(np.expand_dims(X_mean0, 2), np.expand_dims(self.inv_covs,1)),  np.expand_dims(X_mean0, 3))).T + self.ln_apriories\n","        return grado_de_pertenencia \n","\n","    def pred_label(self,X):\n","        \"\"\"Estima la etiqueta de cada dato. La etiqueta puede ser un entero o bien un string.\n","        X: matriz numpy cada fila es un dato, cada columna una medida\n","        retorna un vector con las etiquetas de cada dato\"\"\"\n","        print(\"X shape\", X.shape)\n","        return self.labels[np.argmax(X, axis=1)]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kf237UdHVMPd","colab_type":"text"},"source":["### **Resultados**\n","\n","| Base de datos | Número de aciertos | Porcentaje de aciertos |\n","| --- | --- | --- |\n","| Iris   | 147 | 98.0 |\n","| Wine   | 177 | 99.4382 |\n","| Cancer | 554 | 97.3637 |\n","| MNIST  | - | - |\n","| Isolet | - | - |\n","\n","\n","Como se puede observar en los resultados, se alcanzan mejores porcentajes en este caso que con el clasificador de la distancia euclídea, menos para los dataset MNIST e Isolet, dónde no se puede aplicar el clasificador estadístico bayesiano. \n","\n","Como ambos dataset cuentan con un número elevados de parámetros respecto al número de datos, ocurre un error de matriz singular (no invertible) al calcularse la inversa de la matriz de covarianzas en el entrenamiento, haciendo imposible el uso de este clasificador sin algún método de regularización."]},{"cell_type":"markdown","metadata":{"id":"Cv7PRyHJ9QhV","colab_type":"text"},"source":["# **4. Regularización**\n","\n","Cuando en el dataset que utilizamos tiene pocos datos y un número elevado de características discriminantes, se comparte la matriz de covarianzas para todas las clases, ya que la función discriminante lineal $Σ_i = Σ_j$ puede dar mejor rendimiento, debido a que emplean menos parámetros.\n","\n","En el caso de que se dispongan muy pocos datos, se puede asumir que $Σ_i = Σ_j = \\sigma^2I$, donde $I$ es la matriz identidad, en cuyo caso el clasificador resultante es bastante similar al de la distancia euclídea.\n","\n","El proceso de modificar las matrices de covarianzas para mejorar el rendimiento de nuestro clasificador se denomina regularización, y es adecuado cuando se dispone de pocos datos y un número elevado de parámetros. Se dispone de dos parámetros:\n","- $\\lambda$: el grado de igualdad entre las $Σ_i$\n","- $\\gamma$: el grado de semejanda a la matriz identidad de las $Σ_i$.\n","\n","Se deben seleccionar los valores de $\\lambda$ y $\\gamma$ que optimizan el rendimiento del clasificador.\n","\n","Para la implementación de la **regularización** en el clasificador estadístico bayesiano se han utilizado los parámetros:\n","- **share_covs**: indica si se comparte la matriz de covarianzas entre las clases o no.\n","- **shrinkage**: determina el grado de diagonalidad de la matriz de covarianzas.\n","\n"]},{"cell_type":"code","metadata":{"id":"7NVOH3CRHCf-","colab_type":"code","colab":{}},"source":["import numpy as np\n","from sklearn.covariance import ShrunkCovariance\n","from sklearn import preprocessing\n","from abc import abstractmethod\n","import pandas as pd\n","import numpy as np\n","\n","\n","class Classifier:\n","\n","    @abstractmethod\n","    def fit(self,X,y):\n","        pass\n","\n","    @abstractmethod\n","    def predict(self,X):\n","        pass\n","\n","class ClassifBayesianoParametrico(Classifier):\n","    def __init__(self, share_covs=False, shrinkage=0.0):\n","        \"\"\"Constructor de la clase\n","        share_covs: Indica si la matriz de covarianzas va a ser compartida entre las distintas clases.\n","        shrinkage: Parámetro que determina la diagonalidad de la matriz de covarianzas. Ver sklearn.covariance.ShrunkCovariance\n","        \"\"\"\n","        assert 0 <= shrinkage <= 1\n","        self.labels = None\n","        self.ln_apriories = None\n","        self.means = None\n","        self.ln_determinants = None\n","        self.inv_covs = None\n","        self.share_covs = share_covs\n","        self.shrinkage = shrinkage\n","        self.scaler = preprocessing.StandardScaler()\n","\n","    def fit(self, X, y):\n","        \"\"\"Entrena el clasificador\n","        X: matriz numpy cada fila es un dato, cada columna una medida\n","        y: vector de etiquetas, tantos elementos como filas en X\n","        retorna objeto clasificador\"\"\"\n","        assert X.ndim == 2 and X.shape[0] == len(y)\n","        # Aseguramos que las etiquetas son numeros tal que: [0, 1, ..., N]\n","        y = pd.factorize(y)[0]\n","        # Preprocesamos los datos de entrada\n","        X = self.scaler.fit_transform(X)\n","        # Contar cuantos ejemplos hay de cada etiqueta\n","        self.labels, labels_count = np.unique(y, return_counts=True)\n","        # Usando el contador de ejemplos de cada etiqueta, calcular el logaritmo neperiano de las probabilidades a-priori\n","        self.ln_apriories = np.log(labels_count/y.size)\n","        # Calcular para los ejemplos de cada clase, la media de cada una de sus características (centroide)\n","        self.means = np.array([np.mean(X[y == i], axis=0) for i in np.unique(y)])\n","        \n","        if self.share_covs:\n","            # Restamos al dato de cada clase su centroide\n","            xmz = np.array([])\n","            for l in self.labels:\n","              xmz = np.append(xmz.reshape(-1, X.shape[1]), X[y==l] - self.means[l], axis=0)\n","            # Calcula la matriz de covarianzas empleando la clase ShrunkCovariance de sklearn\n","            cov =  ShrunkCovariance(shrinkage=self.shrinkage).fit(xmz).covariance_\n","            #cov = ShrunkCovariance(shrinkage=self.shrinkage).fit(xmz).get_precision()\n","            # La reproducimos tantas veces como número de clases\n","            covs = np.tile(cov, (len(self.labels), 1, 1))\n","            print(\"Covarianzas conjuntas: \", covs, covs.shape)\n","        else:\n","            # Calcula la matriz de covarianzas empleando la clase ShrunkCovariance de sklearn\n","            covs = np.array([ShrunkCovariance(shrinkage=self.shrinkage).fit(X[y==np.unique(y)[i]]-self.means[i]).covariance_ for i in range(len(self.labels))])\n","            print(\"Covarianzas por separado: \", covs, covs.shape)\n","\n","        \n","        # Para cada una de las clases, calcular el logaritmo neperiano de su matriz de covarianzas (puedes emplear compresión de listas o la función map)\n","        self.ln_determinants = np.log(np.linalg.det(covs))\n","        if np.any(np.isinf(self.ln_determinants)):\n","          print(\"Warning: There is a covariance matrix with determinant equal zero\")\n","        # Para cada una de las clases, calcular la inversa de su matriz de covarianzas (puedes emplear compresión de listas o la función map)\n","        self.inv_covs = np.linalg.inv(covs)\n","        \n","        return self\n","\n","    def predict(self, X):\n","        \"\"\"Estima el grado de pertenencia de cada dato a todas las clases\n","        X: matriz numpy cada fila es un dato, cada columna una medida del vector de caracteristicas.\n","        Retorna una matriz, con tantas filas como datos y tantas columnas como clases tenga\n","        el problema, cada fila almacena los valores pertenencia de un dato a cada clase\"\"\"\n","        assert self.means is not None, \"Error: The classifier needs to be fitted. Please call fit(X, y) method.\"\n","        assert X.ndim == 2 and X.shape[1] == self.means.shape[1]\n","\n","        # Preprocesamos nuestros datos\n","        X = self.scaler.fit_transform(X) \n","\n","        # Resta la media de cada clase a cada ejemplo en X\n","        X_mean0 = np.array([X-self.means[i] for i in range(len(self.means))])\n","        \n","        # Calcula el logaritmo de la función de decisión gausiana\n","        # Transparencias de Métodos paramétricos de clasificación: página 14:\n","        # -(1/2)ln|Sigma_i| - (1/2)*(x- mu_i)^T Sigma_i^-1 (x- mu_i) + lnP(alpha_i)\n","        grado_de_pertenencia = -(1/2)*self.ln_determinants - (1/2)*np.squeeze(np.matmul(np.matmul(np.expand_dims(X_mean0, 2), np.expand_dims(self.inv_covs,1)),  np.expand_dims(X_mean0, 3))).T + self.ln_apriories\n","        return grado_de_pertenencia\n","\n","    def pred_label(self, X):\n","        \"\"\"Estima la etiqueta de cada dato. La etiqueta puede ser un entero o bien un string.\n","        X: matriz numpy cada fila es un dato, cada columna una medida\n","        retorna un vector con las etiquetas de cada dato\"\"\"\n","        return self.labels[np.argmax(X, axis=1)]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"beemtYv6HIfu","colab_type":"text"},"source":["### **Resultados:**\n","\n","| Base de datos | Número de aciertos | Porcentaje de aciertos |\n","| --- | --- | --- |\n","| Iris   | 147 | 98.0 |\n","| Wine   | 178 | 100 |\n","| Cancer | 555 | 97.53953 |\n","| MNIST  | 52289 | 87.14833 |\n","| Isolet | 7488 | 96.03693 |\n","\n","Como se puede observar en la tabla, los resultado se ven mejorados respecto a la implementación del clasificador estadístico bayesiano sin regularización. \n","\n","Además, al compartir la matriz de covarianzas en las bases de datos MNIST e Isolet se consigue evitar el error obtenido previamente, haciendo las matrices de covarianzas invertibles a través de este tipo de transformación. \n","\n","Los resultados obtenidos se deben a los siguiente valores de los parámetros:\n","\n","| Base de datos | share_covs | shrinkage |\n","| --- | --- | --- |\n","| Iris   | False | 0 |\n","| Wine   | True | 0 |\n","| Cancer | False | 0 |\n","| MNIST  | True | 0,3 |\n","| Isolet | True | 0,25 |\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Z2064dpDIWNd","colab_type":"text"},"source":["# **5. Evaluación del Rendimiento**\n","\n","## **5.1 Validación Cruzada**\n","\n","Para los dataset Iris, Wine y Cancer se ha evaluado el rendimiento del clasificador de la distancia euclídea y el calsificador estadísitico bayesiano con regularización, mediante **validación cruzada _k-fold_**, que consiste en dividir el dataset en k porciones iguales, entrenandolo con k-1 porciones de datos, y evaluandolo con la porcion de datos restante, cambiando en cada iteración la porción del dataset utilizada para entrenar.\n","\n","Se ha utilizado la función **_cross_val_score_** con **k = 5** para su implementación en Python en el caso del clasificador euclídeo, midiendo la precisión (accuracy) media para cada dataset.\n","\n","### **Resultados:**\n","\n","- **Iris:**\n","  - Clasificador de la distancia euclídea:\n","    - Accuracy: 0.91 (+/- 0.14)\n","- **Wine:**\n","  - Clasificador de la distancia euclídea:\n","    - Accuracy: 0.73 (+/- 0.12)\n","- **Cancer:**\n","  - Clasificador de la distancia euclídea:\n","    - Accuracy: 0.89 (+/- 0.02)\n","\n","\n","Como se puede observar todos los resultados son inferiores a los obtenidos mediante el uso del clasificador bayesiano. Esta disminución de la precisión de nuestros clasificadores se debe a que se ha entrenado y probado los clasificadores cada vez con un conjunto de datos distinto en vez de con el mismo conjunto de datos.\n","\n","\n","## **5.2 Wrapper**\n","\n","Para mejorar los parámetros externos del clasificador bayesiano, tuvimos que recurrir a los métodos Wrapper. En este caso específico, utilizamos  un método al que llamamos **_best_shrinkage_**, el cual prueba los distintos valores de los parámetros externos **_share_covs_**, para que las clases compartan o no la matriz de covarianzas, y **_shrinkage_**, para definir la diagonalización de la matriz de covarianzas. \n","\n","Este proceso parte de unas listas de valores y de los datos de entrenamiento de la base de datos que se quiere probar que se le pasa como argumentos de entrada. Mediante la función **_GridSearchCV_**, se aplica a su vez el concepto del apartado anterior de validación cruzada, obteniendose la precisión media de las distintas combinaciones de conjuntos de datos y eligiendo los parámetros para la más óptima.\n","\n","### **Resultados:**\n","\n","- **Iris:**\n","  - Clasificador estadístico bayesiano regularizado:\n","    - Accuracy: 0.973 (+/- 0.013)\n","    - shrinkage = 0\n","    - share_covs = False\n","- **Wine:**\n","  - Clasificador estadístico bayesiano regularizado:\n","    - Accuracy: 0.955 (+/- 0.023)\n","    - shrinkage = 0\n","    - share_covs = True\n","- **Cancer:**\n","  - Clasificador estadístico bayesiano regularizado:\n","    - Accuracy: 0.961 (+/- 0.013)\n","    - shrinkage = 0.5\n","    - share_covs = True\n","\n","En este caso se puede observar una situación similar a la de antes al obtenerse la precisión media, solo que a su vez se elige el mejor valor de los parámetros externos (share_covs y shrinkage) comparando las precisiones obtenidas para todas las combinaciones y quedándose con la mayor media.\n","\n","## **5.3 Exclusión**\n","\n","Para los dataset Isolet y MNIST, debido al tamaño de la base datos, se utiliza exclusión como método de evaluación, que significa que se cogen unos subconjuntos de datos del dataset, en vez de utilizar todo el dataset para realizar la validación cruzada, ya que al ser conjuntos de datos tan grandes aplicar este concepto conllevaría un excesivo tiempo de computo.\n","\n","### **Resultados:**\n","\n","- **Isolet:**\n","  - Clasificador de la distancia euclídea:\n","    - Accuracy: 0.847\n","  - Clasificador estadístico bayseiano regularizado:\n","    - Accuracy: 0.939 (+/- 0.000)\n","    - shrinkage = 0.3\n","    - share_covs = True\n","- **MNIST:**\n","  - Clasificador de la distancia euclídea:\n","    - Accuracy: 0.820\n","  - Clasificador estadístico bayseiano regularizado:\n","    - Accuracy: 0.873 (+/- 0.000)\n","    - shrinkage = 0.2\n","    - share_covs = True\n","\n","En este caso se puede observar como la precisión de los clasificadores ha aumentado respecto a la calculada para el dataset completo en secciones anteriores. Esto se debe a que, a pesar dividir el dataset, al obtenerse el mejor valor de los parámetros para cada caso, la precisión aumenta respecto al uso exclusivo del clasificador bayesiano regularizado.\n","\n"]},{"cell_type":"code","metadata":{"pycharm":{"is_executing":false,"name":"#%%\n"},"id":"CNSgL7IN5u86","colab_type":"code","colab":{}},"source":["def best_shrinkage_clf(X, y, k, shrinkages, share_covs):\n","    \"\"\"\n","    Busca el clasificador bayesiano regularizado con el mejor shrinkage. \n","    :param X: Ejemplos de la dase de datos\n","    :param y: Etiquetas de los ejemplos\n","    :param k: Número de divisiones en la validación cruzada (k-fold)\n","    :param shrinkages: Lista de posibles shrinkages que conforman la rejilla de búsqueda\n","    :param share_covs: Lista de posibles valores para share_covs que conforman la rejilla de búsqueda\n","    \"\"\"\n","    from sklearn.model_selection import GridSearchCV\n","    cbp = ClassifBayesianoParametrico(share_covs)\n","    params = {'shrinkage': shrinkages, 'share_covs': share_covs}\n","    clf = GridSearchCV(cbp, params, n_jobs=-2, scoring='accuracy', cv=k).fit(X, y)\n","    best_clf = clf.best_estimator_\n","    # print(\"Srinkage scores: \", clf.cv_results_['mean_test_score'])\n","    result_score_mean = clf.cv_results_['mean_test_score'][clf.best_index_]\n","    result_score_std = clf.cv_results_['std_test_score'][clf.best_index_]\n","    print(\"\\tSelected shrinkage = {}, share_covs = {}\\n\" \\\n","          \"\\tAccuracy: {:.3f} (+/- {:.3f})\".format(best_clf.shrinkage,\n","                                                   best_clf.share_covs,\n","                                                   result_score_mean,\n","                                                   result_score_std))\n","    return best_clf"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"49LKafeXxRwK","colab_type":"text"},"source":["# **6.  Aplicación en un caso real de reconocimiento de texto**\n","\n","En esta entrega se aplicó el conocimiento adquirido en las anteriores para, a partir de un conjunto de datos generado por nosotros, usar los clasificadores para reconocer texto de una imagen real. Los cambios realizados sobre el esqueleto del código proporcionado se muestran en los siguientes apartados.\n","\n","### **6.1 Clasificador**\n","\n","Para este caso, se ha decidido usar el clasificador de la distancia euclídea, ya que los resultados obtenidos eran óptimos. \n","\n","### **6.2 Pipeline**\n","\n","A la hora de entrenar el clasificador, se ha utilizado un pipeline al que se le añaden varios filtros de selección de características: \n","- VarianceThreshold:  Descarta los datos con varianza 0, en este caso alrededor de 300 características.\n","- SelectKBest: Descarta el 5% de las características del dataset que son peores, para que no influyan en el entrenamiento del clasificador.\n"]},{"cell_type":"code","metadata":{"id":"CGnZVOXc2JNR","colab_type":"code","colab":{}},"source":["n_vt_cols = VarianceThreshold().fit(X_train, y_train).transform(X_train).shape[1]\n","clf = Pipeline([('feature_selection1',VarianceThreshold()), \n","                ('feature_selection2',SelectKBest(chi2, k=round(n_vt_cols*0.95))),\n","                ('classification',ClassifEuclid())])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UamY2X5e2Qrw","colab_type":"text"},"source":["### **6.3 Tamaño de imagen y texto**\n","\n","Para optimizar la lectura de la imagen se amplió su anchura en píxeles de 2500 a 5000, de manera que ésta fuera más nítida y fácil de reconocer, aunque requiriera más computo.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"X_CMr-gA3e4q","colab_type":"code","colab":{}},"source":["IMG_WIDTH = 5000.0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AGhxBcWM3fEy","colab_type":"text"},"source":["Después, adaptamos parámetros del código para que fueran acordes a este cambio, como el contorno mínimo para reconocer las letras (de 120 a 500)."]},{"cell_type":"code","metadata":{"id":"RlAwX-Kw3fVc","colab_type":"code","colab":{}},"source":["contours = contours[np.array(list(map(cv2.contourArea, contours))) > 500]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SPPm_vdy3fbC","colab_type":"text"},"source":["También se ha cambiado el grosor de los carácteres para hacer que se separen mejor unas letras de otras y no se reconozcan varias como un solo carácter."]},{"cell_type":"code","metadata":{"id":"D3UetT4t4q5P","colab_type":"code","colab":{}},"source":["bin_img = cv2.dilate(bin_img, np.ones((2, 2), np.uint8))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B6tlnbQz4wNq","colab_type":"text"},"source":["Por último, en la parte donde se busca la separación de las letras para evitar reconocer varios carácteres como uno solo, se ha añadido un bucle extra para realizar una doble comprobación, obteniendose mejores resultados. Además, se han eliminado los carácteres que están por debajo de un área definida, en este caso por debajo de 0.66."]},{"cell_type":"code","metadata":{"id":"4FEREQGY8QOL","colab_type":"code","colab":{}},"source":["        for box in char_boxes:\n","            n_splits = round((box[2] / box[3]) / CHAR_ASPECT_RATIO)\n","            if n_splits > 1:\n","                # We have detected a box that clearly contains two chars\n","                char_boxes.remove(box)\n","                for i in range(n_splits):\n","                    char_boxes.append([int(box[0] + (box[2] / n_splits) * i), box[1], int(box[2] / n_splits), box[3]])\n","            elif n_splits < 0.66:\n","                char_boxes.remove(box)\n","                \n","\n","        for box in char_boxes:\n","            n_splits = round((box[2] / box[3]) / CHAR_ASPECT_RATIO)\n","            if n_splits > 1:\n","                # We have detected a box that clearly contains two chars\n","                char_boxes.remove(box)\n","                for i in range(n_splits):\n","                    char_boxes.append([int(box[0] + (box[2] / n_splits) * i), box[1], int(box[2] / n_splits), box[3]])\n","            elif n_splits < 0.66:\n","                char_boxes.remove(box)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qhxtZDG98beX","colab_type":"text"},"source":["### **6.4 Dataset**\n","\n","Se ha modificado la base de datos inicial, añadiendo el carácter \"/\" al no estar previamente y aparecer en el texto. Se probó a añadir los carácteres de interrogación, pero nos empeoraba el reconocimiento más de lo que lo mejoraba al reconocer otros carácteres como interrogaciones, por lo que decidimos no incluirlos en el diccionario de carácteres.\n","\n","Además, se ha ampliado el número de datos generados para entrenar y testear, de manera que el clasificador tenga más datos de entrada y logre un rendimiento más óptimo. Como observación, en el grupo de los datos de entrenamiento se han metido datos con ruido y datos sin ruido, de manera que el conjunto de datos de entrenamiento sea lo más variado posible. \n"]},{"cell_type":"code","metadata":{"id":"n6oxX_FZAHVD","colab_type":"code","colab":{}},"source":["labels = np.array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H',\n","                   'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n","                   '/'\n","                   ])\n","\n","font_filename = os.path.join(resources_dir, \"AndaleMono.ttf\")\n","X_train1, y_train1 = generate_ttf_images(font_filename, 90, labels, True)\n","X_train2, y_train2 = generate_ttf_images(font_filename, 10, labels, False)\n","X_train = np.append(X_train1, X_train2, axis=0)\n","y_train = np.append(y_train1, y_train2)\n","X_train = X_train.reshape(len(X_train), -1).astype(float)\n","\n","print(\"--> Generating test dataset\")\n","X_test, y_test = generate_ttf_images(font_filename, 25, labels, True)\n","X_test = X_test.reshape(len(X_test), -1).astype(float)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yKry915kAVHY","colab_type":"text"},"source":["Los parámetros de la función que añade ruido al conjunto de datos de entrenamiento han sido elegidos de manera que, el conjunto quede lo más variado posible para que se pueda adaptar a varias imagenes de entrada con diferentes ángulos."]},{"cell_type":"code","metadata":{"id":"chq17iq7AXUq","colab_type":"code","colab":{}},"source":[" if add_noise==True:\n","              # Add noise\n","              img = add_noise_to_img(img, rot_noise_level=2, scale_noise_level=0.04, gauss_noise_level=3, blur_noise_level=3, salt_level=1, dilate_level=6)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zzezysd6GaYB","colab_type":"text"},"source":["Finalmente, se ha modificado el tamaño en píxeles de las imagenes generadas para entrenar y testear el clasificador, reduciendo el ancho de la misma de 28 píxeles a 25."]},{"cell_type":"code","metadata":{"id":"7HSHjQofHdRT","colab_type":"code","colab":{}},"source":["    @staticmethod\n","    def standardize_char_size(img: np.ndarray, dst_shape: tuple = (28, 25)):\n","        \"\"\"\n","        Centers and scales the input image (img) to fit it in a image of shape dst_shape with a small border.\n","        :param img: The input image\n","        :param dst_shape: The destination shape. This defines also the number of features used in the classifier.\n","        :return: a new image where the input image has been scaled and centered.\n","        \"\"\"\n","        dst = np.zeros(dst_shape, dtype=float)\n","        h, w = img.shape\n","        margin = 2\n","        # Scale the image to fit in the destination shape\n","        scale_factor = (dst_shape[0] - 2 * margin) / h if h > w else (dst_shape[1] - 2 * margin) / w\n","        resized = cv2.resize(img, (int(scale_factor * w), int(scale_factor * h)))\n","        # Convert from white background to black background\n","        resized = 255 - resized.astype(float)\n","        # Paste in the output image\n","        x_margin = (dst_shape[1] - resized.shape[1]) // 2\n","        y_margin = (dst_shape[0] - resized.shape[0]) // 2\n","        dst[y_margin:y_margin + resized.shape[0], x_margin:x_margin + resized.shape[1]] = resized\n","        return dst"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cydUWO9TAuiC","colab_type":"text"},"source":["### **6.5 Filtros**\n","\n","Se han amplicado varios filtros a la imagen de entrada, de manera que fuera lo más limpia y nítida posible para facilitar el reconocimiento de ésta posteriormente:\n","\n","-GaussianBlur: Aplica un filtro de suavizado a la imagen a partir de un kernel pasado como parámetro de entrada\n","\n","-Threshold: Aplica un filtro que fija un umbral binario a partir del cual se elige el pixel de color blanco, de manera que convierte la parte gris externa de los carácteres a blanco, para que la diferencia entre el fondo de la imagen y el texto sea lo más clara posible."]},{"cell_type":"code","metadata":{"id":"J1lKto2CFXc3","colab_type":"code","colab":{}},"source":["# Blur image to remove some noise\n","blur = cv2.GaussianBlur(img, (3, 3), 1)\n","\n","# Binarize image using the smart Otsu binarization method\n","bin_img = cv2.threshold(img, 170, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F5Kin7UNsYSK","colab_type":"text"},"source":["# **7.  Conclusiones**\n","\n","En esta parte de la asignatura, dirigida especialmente a esta práctica, hemos aprendido multitud de conceptos acerca de la parte de Reconocimiento de Formas dentro del campo de la Inteligencia Artificial, muy útiles de cara a un futuro dónde el manejo de técnicas de análisis de datos y su clasificación está cada vez más en auge y más demandado en el mercado. \n","\n","La mezcla de realizar tareas más prácticas tanto a mano como en código han servido de gran ayuda para entender los distintos conceptos, complementándose muy bien entre ellas a partir de los conocimientos adquiridos previamente en clase. Además, ha sido muy útil el feedback recibido tanto en clase como vía e-mail y en tutorías, resolviendose todas nuestras dudas de una manera rápida y precisa por parte tanto de Luis como de Iago.\n","\n","El transcurso de esta práctica se ha hecho muy ameno debido a su dinámica, ya que se empezó desde cero enseñándose los conceptos básicos tanto de Python como de las herramientas que ibamos a usar en este lenguaje (librerías numpy y el paquete de aprendizje scikit-learn), aumentandose la complejidad según se iban realizando entregas y se explicaban los conceptos utilizados en clase. Esto nos ha facilitado mucho seguir de manera semanal la asignatura, consumiendo el tiempo justo en la realización de la práctica y permitiendo trabajar a su vez en el resto de asignaturas sin ningún tipo de problema.\n","\n","Para concluir, en forma de resumen, hemos quedado muy satisfechos con esta parte de la asignatura por todo lo aprendido en la misma, la fluidez de las clases y la evaluación de las distintas partes muy dirigidas hacía el alumno, facilitando el paso por la asignatura por evaluación contínua."]}]}